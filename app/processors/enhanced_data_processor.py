#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆæ•°æ®å¤„ç†å’Œå›¾ç‰‡ä¸‹è½½å·¥å…·
æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼å’Œæ‰¹é‡å¤„ç†åŠŸèƒ½ï¼ˆapp.processors ç‰ˆæœ¬ï¼‰
"""

import pandas as pd
import requests
import os
import time
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse
from collections import Counter

import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from app.config import (
    REQUIRED_FIELDS, DOWNLOAD_CONFIG,
    FILENAME_CONFIG, LOG_CONFIG, SORT_CONFIG, SUPPORTED_FORMATS,
    MULTITHREADING_CONFIG, ERROR_LOG_CONFIG
)

# é…ç½®æ—¥å¿—
# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
_log_dir = os.path.dirname(LOG_CONFIG['log_file'])
if _log_dir:
    os.makedirs(_log_dir, exist_ok=True)

logging.basicConfig(
    level=getattr(logging, LOG_CONFIG['log_level']),
    format=LOG_CONFIG['log_format'],
    datefmt=LOG_CONFIG.get('date_format'),
    handlers=[
        logging.FileHandler(LOG_CONFIG['log_file'], encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class EnhancedDataProcessor:
    """å¢å¼ºç‰ˆæ•°æ®å¤„ç†å’Œå›¾ç‰‡ä¸‹è½½ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨"""
        self.required_fields = REQUIRED_FIELDS
        self.supported_formats = SUPPORTED_FORMATS
        self.multithreading_enabled = MULTITHREADING_CONFIG['enable_multithreading']
        self.max_threads = MULTITHREADING_CONFIG['max_download_threads']
        self.download_timeout = MULTITHREADING_CONFIG['download_timeout']
        # çº¿ç¨‹å®‰å…¨çš„é”
        self._progress_lock = threading.Lock()
        self._success_count_lock = threading.Lock()
        self._folders_created_lock = threading.Lock()
        # é”™è¯¯æ—¥å¿—ç›¸å…³
        self._error_log_lock = threading.Lock()
        self.error_log_file = None
        self.error_count = 0
        self._init_error_log()
        # é«˜çº§é”™è¯¯æ—¥å¿—ä¸ç»Ÿè®¡
        self._init_advanced_error_logging()

    def _init_advanced_error_logging(self):
        """
        åˆå§‹åŒ–å¢å¼ºé”™è¯¯æ—¥å¿—ï¼ˆè¯¦ç»†æ—¥å¿—+æ±‡æ€»ç»Ÿè®¡ï¼‰ï¼ŒæŒ‰æ—¥æœŸè½®è½¬
        """
        try:
            log_dir = Path(ERROR_LOG_CONFIG['error_log_dir']) if 'error_log_dir' in ERROR_LOG_CONFIG else Path('logs')
            log_dir.mkdir(parents=True, exist_ok=True)
            date_str = datetime.now().strftime('%Y%m%d')
            self.detailed_log_path = log_dir / f"detailed_errors_{date_str}.log"
            self.summary_log_path = log_dir / f"error_summary_{date_str}.log"
            # ç»Ÿè®¡å®¹å™¨
            self.error_type_counts = Counter()
            self.error_time_buckets = Counter()  # æŒ‰å°æ—¶ç»Ÿè®¡: 'YYYY-MM-DD HH:00'
            self.error_reasons = Counter()       # å¸¸è§åŸå› ç»Ÿè®¡
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¢å¼ºé”™è¯¯æ—¥å¿—å¤±è´¥: {e}")

    def _classify_exception(self, e):
        """å°†å¼‚å¸¸ç±»å‹æ˜ å°„ä¸ºå¯è¯»é”™è¯¯ç±»å‹æ ‡ç­¾"""
        from requests.exceptions import Timeout, ConnectionError, HTTPError
        import errno
        if isinstance(e, (Timeout, ConnectionError)):
            return "ç½‘ç»œè¿æ¥é”™è¯¯"
        if isinstance(e, HTTPError):
            return "HTTPçŠ¶æ€é”™è¯¯"
        if isinstance(e, PermissionError):
            return "æ–‡ä»¶æƒé™é”™è¯¯"
        if isinstance(e, FileNotFoundError):
            return "æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯"
        if isinstance(e, OSError) and getattr(e, 'errno', None) == errno.EACCES:
            return "æ–‡ä»¶æƒé™é”™è¯¯"
        if isinstance(e, ValueError):
            return "URLæ ¼å¼é”™è¯¯"
        return "æœªçŸ¥é”™è¯¯"

    def _log_detailed_error(self, error_type, context, exc):
        """
        å†™å…¥è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼ˆæ— å †æ ˆï¼Œä»…å…³é”®ä¿¡æ¯ï¼Œä¾¿äºå®šä½Excelæ•°æ®è¡Œï¼‰ã€‚
        æœŸæœ›åŒ…å«ï¼šæ—¶é—´ã€é”™è¯¯ç±»å‹ã€Excelè¡Œå·ä¸æ•°æ®è¡Œå·ã€è´¦æˆ·/ç”¨æˆ·/å…¬å¸ã€image_urlã€titleã€ç›®æ ‡æ–‡ä»¶ã€å¼‚å¸¸ç®€è¿°ã€‚
        """
        try:
            ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            # è¯»å–ä¸Šä¸‹æ–‡
            excel_row = context.get('excel_row', 'N/A')
            data_row = context.get('row', 'N/A')
            file_path = context.get('file_path', '')
            row_data = context.get('row_data', {}) or {}
            account_id = row_data.get('account_id', 'N/A')
            user_id = row_data.get('user_id', 'N/A')
            company_id = row_data.get('company_id', 'N/A')
            image_url = row_data.get('image_url', 'N/A')
            title = row_data.get('title', '')
            create_time = row_data.get('create_time', 'N/A')

            lines = [
                f"[{ts}] [{error_type}] ä¸‹è½½å¤±è´¥",
                f"  åˆ›å»ºæ—¶é—´: {create_time}",
                f"  è´¦æˆ·ID: {account_id}    ç”¨æˆ·ID: {user_id}    å…¬å¸ID: {company_id}",
                f"  å›¾ç‰‡URL: {image_url}",
                f"  æ ‡é¢˜: {title}",
                f"  ç›®æ ‡æ–‡ä»¶: {file_path}",
                f"  å¼‚å¸¸: {type(exc).__name__}: {exc}",
                "-" * 80,
            ]
            with open(self.detailed_log_path, 'a', encoding='utf-8') as f:
                f.write("\n".join(lines) + "\n")
        except Exception as e:
            logger.error(f"å†™å…¥è¯¦ç»†é”™è¯¯æ—¥å¿—å¤±è´¥: {e}")

    def _accumulate_error_stats(self, error_type, exc):
        """ç´¯è®¡é”™è¯¯ç»Ÿè®¡æ•°æ®"""
        self.error_type_counts[error_type] += 1
        bucket = datetime.now().strftime('%Y-%m-%d %H:00')
        self.error_time_buckets[bucket] += 1
        reason = f"{type(exc).__name__}: {str(exc)[:120]}"
        self.error_reasons[reason] += 1

    def _write_error_summary(self):
        """å°†å½“å‰ç»Ÿè®¡å†™å…¥æ±‡æ€»æ—¥å¿—æ–‡ä»¶"""
        try:
            ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            lines = [
                "=" * 80,
                f"é”™è¯¯æ±‡æ€»å¿«ç…§ @ {ts}",
                "- é”™è¯¯ç±»å‹ç»Ÿè®¡:",
            ]
            for et, cnt in self.error_type_counts.most_common():
                lines.append(f"  â€¢ {et}: {cnt}")
            lines.append("- æ—¶é—´åˆ†å¸ƒ(æŒ‰å°æ—¶):")
            for t, cnt in sorted(self.error_time_buckets.items()):
                lines.append(f"  â€¢ {t}: {cnt}")
            lines.append("- æœ€å¸¸è§é”™è¯¯åŸå› :")
            for r, cnt in self.error_reasons.most_common(5):
                lines.append(f"  â€¢ {r}  Ã—{cnt}")
            # ç®€å•å»ºè®®
            lines.append("- å¤„ç†å»ºè®®:")
            if any(k in self.error_type_counts for k in ["ç½‘ç»œè¿æ¥é”™è¯¯", "HTTPçŠ¶æ€é”™è¯¯"]):
                lines.append("  â€¢ æ£€æŸ¥ç½‘ç»œè¿æ¥/ä»£ç†ï¼Œé€‚å½“å¢å¤§è¶…æ—¶æ—¶é—´ï¼Œå¿…è¦æ—¶é‡è¯•")
            if self.error_type_counts.get("æ–‡ä»¶æƒé™é”™è¯¯", 0) > 0:
                lines.append("  â€¢ æ£€æŸ¥è¾“å‡ºç›®å½•/æ–‡ä»¶æƒé™ï¼Œä½¿ç”¨æœ‰å†™æƒé™çš„è·¯å¾„è¿è¡Œç¨‹åº")
            if self.error_type_counts.get("URLæ ¼å¼é”™è¯¯", 0) > 0:
                lines.append("  â€¢ æ¸…æ´—URLå­—æ®µï¼Œç¡®ä¿ä¸ºæœ‰æ•ˆçš„HTTP(S)é“¾æ¥")
            if self.error_type_counts.get("URLæ— æ•ˆ", 0) > 0:
                lines.append("  â€¢ æ¸…ç†æˆ–ä¿®å¤æ•°æ®ä¸­çš„æ— æ•ˆURLï¼ˆç©ºå€¼ã€'nan'ã€éhttp/httpsï¼‰ï¼Œå¿…è¦æ—¶è·³è¿‡è¯¥è¡Œ")
            with open(self.summary_log_path, 'a', encoding='utf-8') as f:
                f.write("\n".join(lines) + "\n")
        except Exception as e:
            logger.error(f"å†™å…¥é”™è¯¯æ±‡æ€»æ—¥å¿—å¤±è´¥: {e}")

    def clean_logs(self, retention_days=7):
        """
        æ¸…ç†æ—¥å¿—ç›®å½•å†…è¶…è¿‡ä¿ç•™å¤©æ•°çš„æ—¥å¿—æ–‡ä»¶ï¼›retention_days<=0 è¡¨ç¤ºæ¸…ç†å…¨éƒ¨
        è¿”å›åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        try:
            log_dir = Path(ERROR_LOG_CONFIG['error_log_dir']) if 'error_log_dir' in ERROR_LOG_CONFIG else Path('logs')
            if not log_dir.exists():
                return 0
            removed = 0
            now = time.time()
            for fp in log_dir.glob('*errors*.log'):
                if retention_days <= 0:
                    fp.unlink(missing_ok=True)
                    removed += 1
                else:
                    if (now - fp.stat().st_mtime) > (retention_days * 86400):
                        fp.unlink(missing_ok=True)
                        removed += 1
            # å…¼å®¹æ¸…ç†ä¸‹è½½é”™è¯¯txt
            for fp in log_dir.glob('download_errors_*.txt'):
                if retention_days <= 0 or (now - fp.stat().st_mtime) > (retention_days * 86400):
                    fp.unlink(missing_ok=True)
                    removed += 1
            return removed
        except Exception as e:
            logger.error(f"æ¸…ç†æ—¥å¿—å¤±è´¥: {e}")
            return 0

    def _init_error_log(self):
        """åˆå§‹åŒ–é”™è¯¯æ—¥å¿—æ–‡ä»¶"""
        try:
            # åˆ›å»ºé”™è¯¯æ—¥å¿—ç›®å½•
            error_log_dir = Path(ERROR_LOG_CONFIG['error_log_dir'])
            error_log_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆé”™è¯¯æ—¥å¿—æ–‡ä»¶å
            timestamp = datetime.now().strftime(ERROR_LOG_CONFIG['timestamp_format'])
            filename = ERROR_LOG_CONFIG['error_log_filename_template'].format(timestamp=timestamp)
            self.error_log_file = error_log_dir / filename

            # å†™å…¥æ–‡ä»¶å¤´
            with open(self.error_log_file, 'w', encoding='utf-8') as f:
                f.write(f"ä¸‹è½½é”™è¯¯æ—¥å¿— - å¼€å§‹æ—¶é—´: {datetime.now().strftime(ERROR_LOG_CONFIG['log_timestamp_format'])}\n")
                f.write("=" * 80 + "\n\n")

            logger.info(f"é”™è¯¯æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {self.error_log_file}")

        except Exception as e:
            logger.error(f"åˆ›å»ºé”™è¯¯æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            self.error_log_file = None

    def _log_download_error(self, row_num, row_data, file_path, error_msg):
        """è®°å½•ä¸‹è½½é”™è¯¯åˆ°æ–‡ä»¶"""
        if not self.error_log_file:
            return

        try:
            with self._error_log_lock:
                self.error_count += 1

                # å‡†å¤‡é”™è¯¯ä¿¡æ¯
                timestamp = datetime.now().strftime(ERROR_LOG_CONFIG['log_timestamp_format'])
                url = str(row_data.get('image_url', 'N/A'))
                if len(url) > 100:
                    url = url[:100] + '...'

                error_line = ERROR_LOG_CONFIG['error_log_format'].format(
                    timestamp=timestamp,
                    row_num=row_num,
                    account_id=row_data.get('account_id', 'N/A'),
                    user_id=row_data.get('user_id', 'N/A'),
                    company_id=row_data.get('company_id', 'N/A'),
                    create_time=row_data.get('create_time', 'N/A'),
                    url=url,
                    file_path=file_path,
                    error=str(error_msg)
                )

                # å†™å…¥é”™è¯¯æ—¥å¿—æ–‡ä»¶
                with open(self.error_log_file, 'a', encoding='utf-8') as f:
                    f.write(error_line + '\n')

        except Exception as e:
            logger.error(f"å†™å…¥é”™è¯¯æ—¥å¿—å¤±è´¥: {e}")

    def get_error_log_summary(self):
        """è·å–é”™è¯¯æ—¥å¿—æ‘˜è¦"""
        return {
            'error_count': self.error_count,
            'error_log_file': str(self.error_log_file) if self.error_log_file else None
        }

    def detect_file_format(self, file_path):
        """
        æ£€æµ‹æ–‡ä»¶æ ¼å¼

        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„

        Returns:
            str: æ–‡ä»¶æ ¼å¼ç±»å‹ ('excel', 'csv', 'unknown')
        """
        file_extension = Path(file_path).suffix.lower()
        return self.supported_formats.get(file_extension, 'unknown')

    def load_data_file(self, file_path):
        """
        æ ¹æ®æ–‡ä»¶æ ¼å¼åŠ è½½æ•°æ®

        Args:
            file_path (str): æ–‡ä»¶è·¯å¾„

        Returns:
            pandas.DataFrame or None: åŠ è½½çš„æ•°æ®
        """
        try:
            file_format = self.detect_file_format(file_path)
            logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶æ ¼å¼: {file_format} - {file_path}")

            if file_format == 'excel':
                df = pd.read_excel(file_path)
            elif file_format == 'csv':
                # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼
                encodings = ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']
                df = None
                for encoding in encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding)
                        logger.info(f"æˆåŠŸä½¿ç”¨ç¼–ç  {encoding} è¯»å–CSVæ–‡ä»¶")
                        break
                    except UnicodeDecodeError:
                        continue

                if df is None:
                    raise ValueError(f"æ— æ³•è¯»å–CSVæ–‡ä»¶ï¼Œå°è¯•äº†ç¼–ç : {encodings}")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")

            logger.info(f"æ–‡ä»¶åŠ è½½æˆåŠŸ: {file_path}, æ•°æ®è¡Œæ•°: {len(df)}")
            return df

        except Exception as e:
            logger.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return None

    def validate_data_fields(self, df, file_path):
        """
        éªŒè¯æ•°æ®å­—æ®µæ˜¯å¦å®Œæ•´

        Args:
            df (pandas.DataFrame): æ•°æ®æ¡†
            file_path (str): æ–‡ä»¶è·¯å¾„

        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            missing_fields = [field for field in self.required_fields if field not in df.columns]
            if missing_fields:
                logger.error(f"æ–‡ä»¶ {file_path} ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
                logger.info(f"æ–‡ä»¶ç°æœ‰å­—æ®µ: {df.columns.tolist()}")
                return False

            logger.info(f"æ–‡ä»¶ {file_path} å­—æ®µéªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"å­—æ®µéªŒè¯å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False

    def process_single_file(self, input_file_path, output_folder=None, account_filter=None):
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶

        Args:
            input_file_path (str): è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file_path}")

            # åŠ è½½æ•°æ®
            df = self.load_data_file(input_file_path)
            if df is None:
                return False

            # éªŒè¯å­—æ®µ
            if not self.validate_data_fields(df, input_file_path):
                return False

            # æå–æ‰€éœ€å­—æ®µ
            processed_df = df[self.required_fields].copy()

            # æ’åº
            processed_df = processed_df.sort_values(
                by=SORT_CONFIG['sort_by'],
                ascending=SORT_CONFIG['ascending']
            ).reset_index(drop=True)

            # å¯é€‰ï¼šæŒ‰ account_id è¿‡æ»¤
            if account_filter:
                filt = {str(a).strip() for a in account_filter}
                processed_df = processed_df[processed_df['account_id'].astype(str).isin(filt)].reset_index(drop=True)
                logger.info(f"åº”ç”¨è´¦æˆ·ç­›é€‰ï¼Œå…±ä¿ç•™ {len(processed_df)} è¡Œ")

            # æ·»åŠ file_nameåˆ—
            processed_df['file_name'] = ''

            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            input_path = Path(input_file_path)
            if output_folder:
                output_folder_path = Path(output_folder)
                output_folder_path.mkdir(parents=True, exist_ok=True)
                output_file_path = output_folder_path / f"{input_path.stem}_updated.xlsx"
                #     
                #  
                base_folder = output_folder_path
            else:
                # é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶å¤¹
                from app.config import DEFAULT_OUTPUT_FOLDER
                output_folder_path = Path(DEFAULT_OUTPUT_FOLDER)
                output_folder_path.mkdir(parents=True, exist_ok=True)
                output_file_path = output_folder_path / f"{input_path.stem}_updated.xlsx"
                base_folder = output_folder_path

            # åˆ›å»ºåŸºç¡€æ–‡ä»¶å¤¹
            base_folder.mkdir(parents=True, exist_ok=True)

            # æ˜¾ç¤ºå¼€å§‹ä¸‹è½½çš„æç¤ºï¼ˆç»å¯¹è·¯å¾„ï¼‰
            output_abs_path = output_folder_path.resolve()
            base_folder_abs_path = base_folder.resolve()
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_abs_path}")
            logger.info(f"ğŸ’¾ å›¾ç‰‡å°†ä¿å­˜åœ¨: {base_folder_abs_path}")
            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(processed_df)} å¼ å›¾ç‰‡...")

            # ä¸‹è½½å›¾ç‰‡
            success_count = self.download_images_for_dataframe(processed_df, base_folder)

            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            processed_df.to_excel(output_file_path, index=False)

            # æ˜¾ç¤ºå®Œæˆä¿¡æ¯ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
            output_file_abs_path = output_file_path.resolve()

            logger.info("=" * 60)
            logger.info("âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼")
            logger.info("=" * 60)
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_abs_path}")
            logger.info(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {output_file_abs_path}")
            logger.info(f"ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•: {base_folder_abs_path}")
            logger.info(f"ğŸ“Š æˆåŠŸä¸‹è½½: {success_count}/{len(processed_df)} å¼ å›¾ç‰‡")

            # æ˜¾ç¤ºé”™è¯¯æ—¥å¿—æ‘˜è¦
            error_summary = self.get_error_log_summary()
            if error_summary['error_count'] > 0:
                logger.info(f"âŒ ä¸‹è½½é”™è¯¯: {error_summary['error_count']} ä¸ª")
                logger.info(f"ğŸ“‹ é”™è¯¯æ—¥å¿—: {error_summary['error_log_file']}")
            else:
                logger.info("âœ… æ‰€æœ‰å›¾ç‰‡ä¸‹è½½æˆåŠŸï¼Œæ— é”™è¯¯è®°å½•")
            logger.info("=" * 60)

            # å†™å…¥é”™è¯¯æ±‡æ€»æ—¥å¿—å¹¶æç¤ºè¯¦ç»†æ—¥å¿—ä½ç½®
            self._write_error_summary()
            try:
                logger.info(f"è¯¦ç»†é”™è¯¯æ—¥å¿—: {getattr(self, 'detailed_log_path', None)}")
                logger.info(f"é”™è¯¯æ±‡æ€»æ—¥å¿—: {getattr(self, 'summary_log_path', None)}")
            except Exception:
                pass

            return True

        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {input_file_path}, é”™è¯¯: {e}")
            return False

    def download_images_for_dataframe(self, df, base_folder):
        """
        ä¸ºæ•°æ®æ¡†ä¸‹è½½å›¾ç‰‡ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©å•çº¿ç¨‹æˆ–å¤šçº¿ç¨‹ï¼‰

        Args:
            df (pandas.DataFrame): æ•°æ®æ¡†

        #     
        #  URL  
            base_folder (Path): åŸºç¡€æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            int: æˆåŠŸä¸‹è½½çš„å›¾ç‰‡æ•°é‡
        """
        # ç»Ÿè®¡æ— æ•ˆURLè·³è¿‡æ•°é‡ï¼ˆå½“å‰ä¸‹è½½æ‰¹æ¬¡ï¼‰
        self.invalid_url_skipped = 0

        if self.multithreading_enabled and len(df) > 1:
            logger.info("ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½æ¨¡å¼")
            return self.download_images_for_dataframe_multithreaded(df, base_folder)
        else:
            logger.info("ä½¿ç”¨å•çº¿ç¨‹ä¸‹è½½æ¨¡å¼")
            return self.download_images_for_dataframe_singlethreaded(df, base_folder)

    def download_images_for_dataframe_singlethreaded(self, df, base_folder):
        """
        å•çº¿ç¨‹ä¸ºæ•°æ®æ¡†ä¸‹è½½å›¾ç‰‡

        Args:
            df (pandas.DataFrame): æ•°æ®æ¡†
            base_folder (Path): åŸºç¡€æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            int: æˆåŠŸä¸‹è½½çš„å›¾ç‰‡æ•°é‡
        """
        success_count = 0
        total_count = len(df)

        # åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
        folders_created = set()

        for index, row in df.iterrows():
            try:
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ + URLç‰‡æ®µ + å¯é€‰titleï¼Œå¸¦é•¿åº¦ä¸å­—ç¬¦å®‰å…¨å¤„ç†ï¼‰
                filename = self.build_output_filename(row)

                # ç”Ÿæˆå¤šçº§æ–‡ä»¶å¤¹è·¯å¾„ï¼š{company_id}/{account_id}/{user_id}
                company_id = str(row.get('company_id', 'unknown'))
                account_id = str(row.get('account_id', 'unknown'))
                user_id = str(row.get('user_id', 'unknown'))
                folder_path = base_folder / company_id / account_id / user_id

                # åˆ›å»ºæ–‡ä»¶å¤¹
                key = f"{company_id}/{account_id}/{user_id}"
                if key not in folders_created:
                    folder_path.mkdir(parents=True, exist_ok=True)
                    folders_created.add(key)

                file_path = folder_path / filename

                # URLæœ‰æ•ˆæ€§æ ¡éªŒï¼Œè‹¥æ— æ•ˆåˆ™è®°å½•å¹¶è·³è¿‡
                url_val = row.get('image_url', '')
                if not self.is_valid_http_url(url_val):
                    err_type = "URLæ— æ•ˆ"
                    err = ValueError(f"æ— æ•ˆURL: {url_val}")
                    # ç®€è¦é”™è¯¯æ—¥å¿—
                    self._log_download_error(index + 1, row, str(file_path), str(err))
                    # è¯¦ç»†é”™è¯¯æ—¥å¿—ä¸ç»Ÿè®¡
                    row_data = {
                        'account_id': row.get('account_id', 'N/A'),
                        'user_id': row.get('user_id', 'N/A'),
                        'company_id': row.get('company_id', 'N/A'),
                        'image_url': url_val,
                        'title': row.get('title', ''),
                        'create_time': row.get('create_time', 'N/A')
                    }
                    context = {
                        'excel_row': (index + 2),
                        'row': index + 1,
                        'file_path': str(file_path),
                        'row_data': row_data,
                    }
                    self._log_detailed_error(err_type, context, err)
                    self._accumulate_error_stats(err_type, err)
                    logger.error(f"[URLæ— æ•ˆ] ä¸‹è½½è·³è¿‡ - ç¬¬ {index + 1} è¡Œ, URL: {url_val}")
                    df.at[index, 'file_name'] = ''
                    self.invalid_url_skipped += 1
                    # ä¸‹ä¸€è¡Œ
                    continue

                # é‡å¤æ–‡ä»¶æ£€æŸ¥ï¼šåŒåå­˜åœ¨åˆ™è·³è¿‡ä¸‹è½½
                if file_path.exists():
                    logger.info(f"å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½: {file_path}")
                    df.at[index, 'file_name'] = filename
                    continue

                # ä¸‹è½½å›¾ç‰‡
                if self.download_image(url_val, str(file_path)):
                    df.at[index, 'file_name'] = filename
                    success_count += 1
                else:
                    df.at[index, 'file_name'] = ''

                # æ·»åŠ å»¶è¿Ÿ
                time.sleep(DOWNLOAD_CONFIG['delay_between_requests'])

                # æ˜¾ç¤ºè¿›åº¦
                progress = (index + 1) / total_count * 100
                if (index + 1) % 10 == 0 or index == total_count - 1:
                    logger.info(f"ä¸‹è½½è¿›åº¦: {index + 1}/{total_count} ({progress:.1f}%)")

            except Exception as e:
                # è®°å½•é”™è¯¯åˆ°æ—¥å¿—æ–‡ä»¶
                self._log_download_error(index + 1, row, str(file_path), str(e))

                # é”™è¯¯åˆ†ç±»ä¸è¯¦ç»†æ—¥å¿—
                err_type = self._classify_exception(e)
                row_data = {
                    'account_id': row.get('account_id', 'N/A'),
                    'user_id': row.get('user_id', 'N/A'),
                    'company_id': row.get('company_id', 'N/A'),
                    'image_url': row.get('image_url', 'N/A'),
                    'title': row.get('title', ''),
                    'create_time': row.get('create_time', 'N/A')
                }
                context = {
                    'excel_row': (index + 2),  # Excel å®é™…è¡Œå·ï¼ˆå«è¡¨å¤´ï¼‰
                    'row': index + 1,          # æ•°æ®è¡Œå·ï¼ˆ1-basedï¼‰
                    'file_path': str(file_path),
                    'row_data': row_data,
                }
                self._log_detailed_error(err_type, context, e)
                self._accumulate_error_stats(err_type, e)

                # æ§åˆ¶å°è¾“å‡ºï¼ˆå¸¦é”™è¯¯ç±»å‹ï¼‰
                error_info = {
                    'è¡Œå·': index + 1,
                    'è´¦æˆ·ID': row.get('account_id', 'N/A'),
                    'ç”¨æˆ·ID': row.get('user_id', 'N/A'),
                    'å…¬å¸ID': row.get('company_id', 'N/A'),
                    'å›¾ç‰‡URL': row.get('image_url', 'N/A')[:100] + '...' if len(str(row.get('image_url', ''))) > 100 else row.get('image_url', 'N/A'),
                    'é”™è¯¯': f'[{err_type}] {str(e)}'
                }
                logger.error(f"[{err_type}] ä¸‹è½½å¤±è´¥ - ç¬¬ {index + 1} è¡Œæ•°æ®:")
                for key, value in error_info.items():
                    logger.error(f"  {key}: {value}")
                df.at[index, 'file_name'] = ''

        logger.info(f"å•çº¿ç¨‹ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{total_count}ï¼Œæ— æ•ˆURLè·³è¿‡: {self.invalid_url_skipped}")
        return success_count

    def download_images_for_dataframe_multithreaded(self, df, base_folder):
        """
        å¤šçº¿ç¨‹ä¸ºæ•°æ®æ¡†ä¸‹è½½å›¾ç‰‡

        Args:
            df (pandas.DataFrame): æ•°æ®æ¡†
            base_folder (Path): åŸºç¡€æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            int: æˆåŠŸä¸‹è½½çš„å›¾ç‰‡æ•°é‡
        """
        success_count = 0

        # åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
        folders_created = set()

        # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
        download_tasks = []

        for index, row in df.iterrows():
            try:
                # ç”Ÿæˆæ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ + URLç‰‡æ®µ + å¯é€‰titleï¼Œå¸¦é•¿åº¦ä¸å­—ç¬¦å®‰å…¨å¤„ç†ï¼‰
                filename = self.build_output_filename(row)

                # ç”Ÿæˆæ–‡ä»¶å¤¹è·¯å¾„
                # ç”Ÿæˆå¤šçº§æ–‡ä»¶å¤¹è·¯å¾„ï¼š{company_id}/{account_id}/{user_id}
                company_id = str(row.get('company_id', 'unknown'))
                account_id = str(row.get('account_id', 'unknown'))
                user_id = str(row.get('user_id', 'unknown'))
                folder_path = base_folder / company_id / account_id / user_id

                # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                key = f"{company_id}/{account_id}/{user_id}"
                with self._folders_created_lock:
                    if key not in folders_created:
                        folder_path.mkdir(parents=True, exist_ok=True)
                        folders_created.add(key)

                file_path = folder_path / filename

                # URLæœ‰æ•ˆæ€§æ ¡éªŒï¼Œè‹¥æ— æ•ˆåˆ™è®°å½•å¹¶è·³è¿‡
                url_val = row.get('image_url', '')
                if not self.is_valid_http_url(url_val):
                    err_type = "URLæ— æ•ˆ"
                    err = ValueError(f"æ— æ•ˆURL: {url_val}")
                    self._log_download_error(index + 1, row, str(file_path), str(err))
                    row_data = {
                        'account_id': row.get('account_id', 'N/A'),
                        'user_id': row.get('user_id', 'N/A'),
                        'company_id': row.get('company_id', 'N/A'),
                        'image_url': url_val,
                        'title': row.get('title', ''),
                        'create_time': row.get('create_time', 'N/A')
                    }
                    context = {
                        'excel_row': (index + 2),
                        'row': index + 1,
                        'file_path': str(file_path),
                        'row_data': row_data,
                    }
                    self._log_detailed_error(err_type, context, err)
                    self._accumulate_error_stats(err_type, err)
                    logger.error(f"[URLæ— æ•ˆ] ä¸‹è½½è·³è¿‡ - ç¬¬ {index + 1} è¡Œ, URL: {url_val}")
                    df.at[index, 'file_name'] = ''
                    self.invalid_url_skipped += 1
                    continue

                # é‡å¤æ–‡ä»¶æ£€æŸ¥ï¼šåŒåå­˜åœ¨åˆ™è·³è¿‡ä¸‹è½½
                if file_path.exists():
                    logger.info(f"å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½: {file_path}")
                    df.at[index, 'file_name'] = filename
                    continue

                # æ·»åŠ åˆ°ä¸‹è½½ä»»åŠ¡åˆ—è¡¨
                download_tasks.append({
                    'index': index,
                    'url': url_val,
                    'file_path': str(file_path),
                    'filename': filename
                })

            except Exception as e:
                logger.error(f"å‡†å¤‡ç¬¬ {index + 1} è¡Œä¸‹è½½ä»»åŠ¡æ—¶å‡ºé”™: {e}")
                df.at[index, 'file_name'] = ''

        if not download_tasks:
            logger.info("æ²¡æœ‰éœ€è¦ä¸‹è½½çš„å›¾ç‰‡")
            return 0

        logger.info(f"å¼€å§‹å¤šçº¿ç¨‹ä¸‹è½½ {len(download_tasks)} ä¸ªå›¾ç‰‡ï¼Œä½¿ç”¨ {self.max_threads} ä¸ªçº¿ç¨‹")

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œä¸‹è½½
        completed_count = 0
        with ThreadPoolExecutor(max_workers=self.max_threads) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_task = {
                executor.submit(self._download_single_image_task, task): task
                for task in download_tasks
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                completed_count += 1

                try:
                    download_success = future.result()
                    if download_success:
                        df.at[task['index'], 'file_name'] = task['filename']
                        with self._success_count_lock:
                            success_count += 1
                    else:
                        df.at[task['index'], 'file_name'] = ''

                except Exception as e:
                    # è·å–å¯¹åº”è¡Œçš„æ•°æ®ä¿¡æ¯
                    row = df.iloc[task['index']]

                    # è®°å½•é”™è¯¯åˆ°æ—¥å¿—æ–‡ä»¶
                    self._log_download_error(task['index'] + 1, row, task['file_path'], str(e))

                    # é”™è¯¯åˆ†ç±»ä¸è¯¦ç»†æ—¥å¿—
                    err_type = self._classify_exception(e)
                    row_data = {
                        'account_id': row.get('account_id', 'N/A'),
                        'user_id': row.get('user_id', 'N/A'),
                        'company_id': row.get('company_id', 'N/A'),
                        'image_url': row.get('image_url', 'N/A'),
                        'title': row.get('title', ''),
                        'create_time': row.get('create_time', 'N/A')
                    }
                    context = {
                        'excel_row': (task['index'] + 2),  # Excel å®é™…è¡Œå·
                        'row': task['index'] + 1,          # æ•°æ®è¡Œå·ï¼ˆ1-basedï¼‰
                        'file_path': task['file_path'],
                        'row_data': row_data,
                    }
                    self._log_detailed_error(err_type, context, e)
                    self._accumulate_error_stats(err_type, e)

                    error_info = {
                        'è¡Œå·': task['index'] + 1,
                        'è´¦æˆ·ID': row.get('account_id', 'N/A'),
                        'ç”¨æˆ·ID': row.get('user_id', 'N/A'),
                        'å…¬å¸ID': row.get('company_id', 'N/A'),
                        'å›¾ç‰‡URL': row.get('image_url', 'N/A')[:100] + '...' if len(str(row.get('image_url', ''))) > 100 else row.get('image_url', 'N/A'),
                        'ç›®æ ‡æ–‡ä»¶': task['file_path'],
                        'é”™è¯¯': f'[{err_type}] {str(e)}'
                    }
                    logger.error(f"[{err_type}] å¤šçº¿ç¨‹ä¸‹è½½å¤±è´¥ - ç¬¬ {task['index'] + 1} è¡Œæ•°æ®:")
                    for key, value in error_info.items():
                        logger.error(f"  {key}: {value}")
                    df.at[task['index'], 'file_name'] = ''

                # æ˜¾ç¤ºè¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                with self._progress_lock:
                    progress = completed_count / len(download_tasks) * 100
                    if completed_count % 10 == 0 or completed_count == len(download_tasks):
                        logger.info(f"ä¸‹è½½è¿›åº¦: {completed_count}/{len(download_tasks)} ({progress:.1f}%)")

        logger.info(f"å¤šçº¿ç¨‹ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(download_tasks)}ï¼Œæ— æ•ˆURLè·³è¿‡: {self.invalid_url_skipped}")
        return success_count

    def _download_single_image_task(self, task):
        """
        å•ä¸ªå›¾ç‰‡ä¸‹è½½ä»»åŠ¡ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰

        Args:
            task (dict): åŒ…å«ä¸‹è½½ä¿¡æ¯çš„å­—å…¸

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            return self.download_image(task['url'], task['file_path'], timeout=self.download_timeout)
        except Exception as e:
            # è¿™é‡Œçš„é”™è¯¯ä¿¡æ¯ä¼šåœ¨ä¸Šå±‚çš„å¤šçº¿ç¨‹å¤„ç†ä¸­æ˜¾ç¤ºæ›´è¯¦ç»†çš„ä¿¡æ¯
            # è¿™é‡Œåªè®°å½•åŸºæœ¬çš„ä¸‹è½½å¤±è´¥ä¿¡æ¯
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {task['file_path']}, é”™è¯¯: {e}")
            return False

    def set_multithreading_enabled(self, enabled):
        """è®¾ç½®æ˜¯å¦å¯ç”¨å¤šçº¿ç¨‹ä¸‹è½½"""
        self.multithreading_enabled = enabled
        logger.info(f"å¤šçº¿ç¨‹ä¸‹è½½å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

    def set_max_threads(self, max_threads):
        """è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°"""
        if max_threads < 1:
            max_threads = 1
        elif max_threads > 20:  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
            max_threads = 20
        self.max_threads = max_threads
        logger.info(f"æœ€å¤§ä¸‹è½½çº¿ç¨‹æ•°è®¾ç½®ä¸º: {max_threads}")

    def set_download_timeout(self, timeout):
        """è®¾ç½®ä¸‹è½½è¶…æ—¶æ—¶é—´"""
        if timeout < 10:
            timeout = 10
        elif timeout > 300:  # é™åˆ¶æœ€å¤§è¶…æ—¶æ—¶é—´
            timeout = 300
        self.download_timeout = timeout
        logger.info(f"ä¸‹è½½è¶…æ—¶æ—¶é—´è®¾ç½®ä¸º: {timeout} ç§’")

    def get_multithreading_status(self):
        """è·å–å¤šçº¿ç¨‹é…ç½®çŠ¶æ€"""
        return {
            'enabled': self.multithreading_enabled,
            'max_threads': self.max_threads,
            'download_timeout': self.download_timeout
        }

    def timestamp_to_filename(self, timestamp):
        """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºæ–‡ä»¶åæ ¼å¼"""
        try:
            dt = datetime.fromtimestamp(timestamp)
            filename = dt.strftime(FILENAME_CONFIG['time_format'])
            return filename
        except Exception as e:
            logger.error(f"æ—¶é—´æˆ³è½¬æ¢å¤±è´¥: {timestamp}, é”™è¯¯: {e}")
            return str(timestamp)


    def sanitize_filename_component(self, text, max_len=None):
        """
        å°†ä»»æ„æ–‡æœ¬è½¬ä¸ºé€‚äºæ–‡ä»¶åçš„å®‰å…¨ç‰‡æ®µï¼š
        - ç§»é™¤æ§åˆ¶å­—ç¬¦
        - æ›¿æ¢éæ³•å­—ç¬¦ / \ : * ? " < > | & % + # ç©ºæ ¼ ç­‰ä¸ºä¸‹åˆ’çº¿
        - å¯é€‰é•¿åº¦æˆªæ–­
        - å»é™¤é¦–å°¾ä¸‹åˆ’çº¿ï¼Œåˆå¹¶é‡å¤ä¸‹åˆ’çº¿
        ä¿ç•™ä¸­æ–‡ã€å­—æ¯ã€æ•°å­—ã€ç‚¹ã€çŸ­æ¨ªçº¿ã€ä¸‹åˆ’çº¿
        """
        import re
        if text is None:
            s = ''
        else:
            s = str(text)
        # å»é™¤æ§åˆ¶å­—ç¬¦
        s = ''.join(ch for ch in s if ord(ch) >= 32)
        # æ›¿æ¢éæ³•å­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        s = re.sub(r"[\\/\:*?\"<>|&%+#\s]+", "_", s)
        # åˆå¹¶è¿ç»­ä¸‹åˆ’çº¿
        s = re.sub(r"_+", "_", s)
        s = s.strip("._ ")
        if max_len and len(s) > max_len:
            s = s[:max_len]
        return s

    def build_output_filename(self, row, max_total_len=200):
        """
        åŸºäºæ—¶é—´æˆ³ã€å®Œæ•´URLã€å¯é€‰titleç”Ÿæˆæœ€ç»ˆæ–‡ä»¶åï¼š
        æ ¼å¼ï¼š{æ—¶é—´æˆ³}_{å¤„ç†åçš„URL}_{å¤„ç†åçš„title}.{æ‰©å±•å}
        - URL ä½¿ç”¨ url å­—æ®µæ¥æ„å»ºæ–‡ä»¶å
        - title ä¸å­˜åœ¨æˆ–ä¸ºç©ºåˆ™è·³è¿‡
        - æ€»é•¿åº¦ä¸è¶…è¿‡ max_total_lenï¼ˆå«æ‰©å±•åï¼‰
        """
        try:
            base = self.timestamp_to_filename(row.get('create_time'))
            # ä½¿ç”¨ url å­—æ®µæ¥æ„å»ºæ–‡ä»¶å
            url = row.get('url', '')
            title = row.get('title', '')
            # è·å–æ‰©å±•åæ—¶ä»ä½¿ç”¨ image_urlï¼ˆå› ä¸ºè¿™æ˜¯å®é™…ä¸‹è½½çš„å›¾ç‰‡URLï¼‰
            ext = self.get_image_extension(row.get('image_url', ''))

            # å»æ‰åè®®å‰ç¼€ï¼ˆhttp://ã€https:// ç­‰ï¼‰åå†åšæ¸…æ´—
            if url:
                import re
                url = re.sub(r'^[a-zA-Z]+://', '', str(url))
            url_part = self.sanitize_filename_component(url, max_len=120)
            title_part = self.sanitize_filename_component(title, max_len=60) if title else ''

            parts = [p for p in [base, url_part, title_part] if p]
            stem = "_".join(parts)

            # æ§åˆ¶æ€»é•¿ï¼ˆåŒ…å«ç‚¹ä¸æ‰©å±•åï¼‰
            allow = max_total_len - len(ext) - 1 if ext else max_total_len
            if allow < 10:
                allow = 10
            if len(stem) > allow:
                stem = stem[:allow]
            filename = f"{stem}{ext}"
            return filename
        except Exception:
            # å›é€€ï¼šä»…æ—¶é—´æˆ³+æ‰©å±•å
            base = self.timestamp_to_filename(row.get('create_time'))
            # è·å–æ‰©å±•åæ—¶ä½¿ç”¨ image_urlï¼ˆå®é™…ä¸‹è½½URLï¼‰ï¼Œæ–‡ä»¶åä½¿ç”¨ url å­—æ®µ
            ext = self.get_image_extension(row.get('image_url', ''))
            return f"{base}{ext}"

    def get_image_extension(self, url):
        """ä»URLè·å–å›¾ç‰‡æ‰©å±•å"""
        try:
            parsed_url = urlparse(url)
            path = parsed_url.path
            extension = os.path.splitext(path)[1]
            return extension if extension else FILENAME_CONFIG['default_extension']
        except:
            return FILENAME_CONFIG['default_extension']

    def download_image(self, url, file_path, timeout=None):
        """ä¸‹è½½å•ä¸ªå›¾ç‰‡"""
        if timeout is None:
            timeout = DOWNLOAD_CONFIG['timeout']

        try:
            headers = {
                'User-Agent': DOWNLOAD_CONFIG['user_agent']
            }

            response = requests.get(url, headers=headers, timeout=timeout, stream=True)
            response.raise_for_status()

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            return True

        except Exception as e:
            err_type = self._classify_exception(e)
            logger.error(f"[{err_type}] å›¾ç‰‡ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
            # è¯¦ç»†æ—¥å¿—ä¸ç»Ÿè®¡
            context = {
                'url': url,
                'file_path': file_path,
                'timeout': timeout,
            }
            self._log_detailed_error(err_type, context, e)
            self._accumulate_error_stats(err_type, e)
            return False


    def is_valid_http_url(self, url: str) -> bool:
        """
        æ ¡éªŒURLæœ‰æ•ˆæ€§ï¼šéç©ºã€é'nan'/'none'/'null'ï¼Œhttp/httpsåè®®ä¸”å«ä¸»æœºã€‚
        """
        try:
            if url is None:
                return False
            s = str(url).strip()
            if not s:
                return False
            if s.lower() in {"nan", "none", "null"}:
                return False
            parsed = urlparse(s)
            if parsed.scheme not in {"http", "https"}:
                return False
            if not parsed.netloc:
                return False
            return True
        except Exception:
            return False

    def find_data_files(self, folder_path, recursive=False):
        """
        æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­çš„æ•°æ®æ–‡ä»¶

        Args:
            folder_path (str): æ–‡ä»¶å¤¹è·¯å¾„
            recursive (bool): æ˜¯å¦é€’å½’æœç´¢

        Returns:
            list: æ‰¾åˆ°çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        folder_path = Path(folder_path)
        if not folder_path.exists():
            logger.error(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return []

        files = []
        patterns = ['*.xlsx', '*.xls', '*.csv']

        for pattern in patterns:
            if recursive:
                files.extend(folder_path.rglob(pattern))
            else:
                files.extend(folder_path.glob(pattern))

        file_paths = [str(f) for f in files]
        logger.info(f"åœ¨æ–‡ä»¶å¤¹ {folder_path} ä¸­æ‰¾åˆ° {len(file_paths)} ä¸ªæ•°æ®æ–‡ä»¶")

        return file_paths

    def process_batch_files(self, input_folder, output_folder=None, recursive=False, account_filter=None):
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶

        Args:
            input_folder (str): è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
            output_folder (str): è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
            recursive (bool): æ˜¯å¦é€’å½’æœç´¢

        Returns:
            dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶")
        logger.info("=" * 60)

        # æ˜¾ç¤ºè¾“å‡ºç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
        if output_folder:
            output_path = Path(output_folder).resolve()
        else:
            from app.config import DEFAULT_OUTPUT_FOLDER
            output_path = Path(DEFAULT_OUTPUT_FOLDER).resolve()

        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
        logger.info(f"ğŸ’¾ æ‰€æœ‰æ–‡ä»¶å°†ä¿å­˜åœ¨: {output_path}")

        # æŸ¥æ‰¾æ–‡ä»¶
        file_paths = self.find_data_files(input_folder, recursive)

        if not file_paths:
            logger.warning(f"åœ¨æ–‡ä»¶å¤¹ {input_folder} ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ•°æ®æ–‡ä»¶")
            return {'total': 0, 'success': 0, 'failed': 0}

        # å¤„ç†ç»Ÿè®¡
        total_files = len(file_paths)
        success_count = 0
        failed_count = 0

        logger.info(f"ğŸš€ æ‰¾åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

        for i, file_path in enumerate(file_paths, 1):
            logger.info(f"å¤„ç†æ–‡ä»¶ {i}/{total_files}: {file_path}")

            if self.process_single_file(file_path, output_folder, account_filter=account_filter):
                success_count += 1
            else:
                failed_count += 1

        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info("=" * 60)
        logger.info("âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
        logger.info(f"  æˆåŠŸå¤„ç†: {success_count}")
        logger.info(f"  å¤„ç†å¤±è´¥: {failed_count}")
        logger.info(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {output_path}")
        logger.info(f"ğŸ–¼ï¸  å›¾ç‰‡ç›®å½•ç»“æ„: {output_path}/company_id/account_id/user_id/")
        logger.info("=" * 60)

        # å†™å…¥ä¸€æ¬¡æ±‡æ€»æ—¥å¿—ï¼Œå¹¶æç¤ºä½ç½®
        self._write_error_summary()
        try:
            logger.info(f"è¯¦ç»†é”™è¯¯æ—¥å¿—: {getattr(self, 'detailed_log_path', None)}")
            logger.info(f"é”™è¯¯æ±‡æ€»æ—¥å¿—: {getattr(self, 'summary_log_path', None)}")
        except Exception:
            pass

        return {
            'total': total_files,
            'success': success_count,
            'failed': failed_count
        }

